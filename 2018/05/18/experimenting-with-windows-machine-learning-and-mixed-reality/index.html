
<!DOCTYPE html>
<html lang="en">
    
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="Meulta&#39;s blog">
    <title>Experimenting with Windows Machine Learning and Mixed Reality - Meulta&#39;s blog</title>
    <meta name="author" content="Etienne Margraff">
    
    
    
        <link rel="alternate" type="application/atom+xml" title="RSS" href="/atom.xml">
    
    <script type="application/ld+json">{"@context":"http://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Etienne Margraff","sameAs":["https://github.com/meulta","https://twitter.com/meulta","https://www.linkedin.com/in/etima/"],"image":"avatar.jpg"},"articleBody":"I have an awesome job at Microsoft: I get to work on mixing AI, Cloud and Mixed Reality experiences with partners and other companies. This enables me to work on cool new stuff and see what companies really want to do. Form all of the Microsoft Cognitive Services : Computer Vision is one that bring a lot to Augmented reality experiences .We&#8217;ve already helped a lot of companies using the Custom Vision online service which you can access via a REST API. The service tells you what’s in the image by providing a list of tags along with their probability ratio. A new feature was added more recently to show you where these objects are located in the image, thanks to the new Object Detection model with bounding boxes.\nThe only piece which was missing is offline support. When you don’t have any internet connectivity, these services are not very useful anymore. When detecting that the device you have is offline, you have two possibilities:\n\nDisabling the features that require this REST API.\nTrying to perform object recognition locally, on the device.\n\nWe recently announced Windows Machine Learning. It is a service in Windows accessible through a set of APIs when you are building Universal Windows Platform (UWP) applications. This service was released at the end of April 2018 in the brand-new Windows update. The good news is: this Windows update is also available on HoloLens!\nThis Windows ML feature requires a trained model in the ONNX format. ONNX is an initiative from companies such as AWS, Facebook and Microsoft to create an open format to represent deep learning models. One way to create an ONNX model today is to convert it from one that already exists. Guess what? Custom Vision gives you the feature of exporting a trained model in ONNX (for some of the model types available).\nI recently worked with a company to try and mix all of this and be able to run a Custom Vision model on HoloLens.\nIt worked! Here is how we did it (using a demo project as an example. You can get the full code here: https://aka.ms/mr-winml-code).\n  winml.mp4\n\nUse Custom Vision to create an ONNX modelCustom Vision (customvision.ai) is one of the Cognitive Services. These are a set of Machine Learning ready-to-use models. The idea is simple: you get the power of Deep Learning and other algorithms without even having to go through the challenge of creating and training the model yourself. If you know how to call a REST API, you know how to use Cognitive Services. Some of these services are customizable. In the Vision API, we have a “Custom” version that you can train by uploading pictures and tagging them.\n  Note: We also recently released a preview of the Object Detection model. It brings you a customizable Vision service WITH bounding boxes! So now not only do you know what is in the picture, you can also know where it is. It is very helpful in Augmented Reality apps as it enables a way to display data on top of objects.\n\nThere is a very comprehensive guide available here explaining how to create a classifier so I am not going to go through it here in detail. Here are some interesting things for you to consider:\nFirst, pick a compact model. If you want to export as ONNX, the model as to be marked as (compact), otherwise you won’t be able to. This makes sense in a way that these models are designed to run offline on devices that do not have a lot of power. It is a good way to insure you have usable exported models.\n\nOnce you’ve picked the model you need, you just have to train it. In my examples, I am training it using photos of a Rubik’s cube and a Seahawks football (for no particular reason, these were just the first things that I found in my office when writing this sample &#x1f60a; ).\n\nYou can start with as few as 10 pictures for each tag and the service will surprise you with its ability to recognize these very accurately. Of course, you can refine the model by adding more pictures with different lightnings, coloring, etc.\nExporting is very easy, go in the Performance tab and click on the Export button. You can choose the format you want and in this case, we need ONNX.\n\nThat’s it. We now have a model we can use offline!\nGenerate the Windows ML wrapperNow that we have a model, we need to write some code that is going to make use of it. You can find everything you need about Windows Machine Learning in the documentation but let’s get through the main actions you need to perform.\nWhen I say that we need to write some code, it is not completely accurate. We will generate a wrapper to use the ONNX model in UWP. The only code we need to write ourselves is the one that gets the image from the webcam and pass it to the wrapper.\nThe Windows ML SDK comes with a tool named MLGen.exe. This command line tool helps you generate a wrapper for an ONNX file. It is located where you installed the Windows SDK. On my computer, which has a standard install, here where it is:\nC:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.17125.0\\x86\\mlgen.exe\n\nThe command line to run is straightforward:\nmlgen -i INPUT-FILE -l LANGUAGE -n NAMESPACE [-o OUTPUT-FILE]\n\n\nINPUT-FILE is the ONNX file.\nLANGUAGE is the programming language (CS in this case).\nNAMESPACE is the namespace used in the CSharp file. This should be something usable in your project, but you can change it later.\nOUTPUT-FILE is the file that is going to be created (a .cs one, in this case).\n\nA generated wrapper contains 3 classes (considering that “ModelName” is the name of your model):\n\nModelName_Input: a structure to hold the input data. In Vision related model, it is a VideoFrame which is easy to get from the Camera in UWP.\nModelName_Ouput: a structure used by the wrapper to give you the output.\nModelName: a class responsible for running the model evaluation. It contains a static constructor and an asynchronous method to evaluate  the model prediction.\n\nHere is an example of generated code:\n1234567891011121314151617181920212223242526272829303132333435363738394041424344454647using System;using System.Collections.Generic;using System.Threading.Tasks;using Windows.Media;using Windows.Storage;using Windows.AI.MachineLearning.Preview;namespace MyNamespace&#123;  public sealed class MyModelInput  &#123;    public VideoFrame data &#123; get; set; &#125;  &#125;public sealed class MyModelOutput&#123;  public IList&lt;string&gt; classLabel &#123; get; set; &#125;  public IDictionary&lt;string, float&gt; loss &#123; get; set; &#125;  public MyModelOutput()  &#123;    this.classLabel = new List&lt;string&gt;();    this.loss = new Dictionary&lt;string, float&gt;();  &#125;&#125;  public sealed class MyModel  &#123;    private LearningModelPreview learningModel;    public static async Task&lt;MyModel&gt; CreateMyModel(StorageFile file)    &#123;      LearningModelPreview learningModel = await LearningModelPreview.LoadModelFromStorageFileAsync(file);      MyModel model = new MyModel();      model.learningModel = learningModel;      return model;    &#125;    public async Task&lt;MyModelOutput&gt; EvaluateAsync(MyModelInput input) &#123;      MyModelOutput output = new MyModelOutput();      LearningModelBindingPreview binding = new LearningModelBindingPreview(learningModel);      binding.Bind(“data“, input.data);      binding.Bind(“classLabel“, output.classLabel);      binding.Bind(“loss“, output.loss);      LearningModelEvaluationResultPreview evalResult = await learningModel.EvaluateAsync(binding, string.Empty);      return output;    &#125;  &#125;&#125;\n  Note: When exporting from the Custom Vision service, the name of the model is a generated one. You might have to change it to something that is more human readable.\n\nAs you can see in this example, the generation tool created an output structure with 2 parameters:\n\nclassLabel: the labels with the highest probability.\nloss: a dictionary with all labels and their respective probability.\n\nThe dictionary is not initialized. If you leave it this way you will get an Exception when running the code. You have to initialize it using the labels you set in the Custom Vision portal. Here is how it looks like for mine:\n12345this.loss = new Dictionary&lt;string, float&gt;()&#123;  &#123; \"football\", 0f &#125;,  &#123; \"rubikscube\", 0f &#125;&#125;;\nAwesome, we now have a usable wrapper. Let see how we can integrate this in a Mixed Reality app.\nIntegrate in your Mixed Reality appThe best way to start integrating your wrapper and ONNX model in an application is to look at the samples that the team is providing. They will guide you through everything you need: starting the Camera, collecting VideoFrames, sending these to the wrapper and getting the result.\nWhen you are creating a Windows Mixed Reality application and, more specifically, a HoloLens app, you use Unity to setup your scene with 3D objects as needed along with C# scripts. This Unity project is then built (or “exported”) as a Visual Studio solution containing a Universal Windows Platform (UWP) project.\n  Note: In theory, a Unity project is meant to be exported to different platforms. We usually try to keep the code as portable as possible. In this specific case we are going to use APIs that are specific to Windows to run the evaluation of the ONNX model. To make sure this platform specific code does not generate errors in Unity, we will wrap it inside conditional compilation keyword. This will tell Unity and Visual Studio: “don’t try to parse / compile this unless you are in UWP”. If you want to run this code on another environment you will have to (at least) add another conditional compilation switch to add the code specific to this platform.\n\nYou can get the full code for this sample here: https://aka.ms/mr-winml-code \nIn the sample project, there is a Game Object with no graphical representation. It is called ScriptHolder and its only role is to have some scripts attached to run code at specific moments during execution. This object has a script named Scene Startup attached to it. This script contains all the code needed to create the ONNX wrapper, get the VideoFrames and display the result of prediction.\nThe Start method in the standard Unity MonoBehaviour is called automatically when the object appears in the scene (i.e. when the application is starting). This will:\n\nGet an instance to the label where we are going to display results to.\nCreate and initialize the MediaCapture object to start collecting frames from the camera.\nInitialize the wrapper for the ONNX model.\n\nInitializing the model is pretty easy, you just load the file from the local storage and call the static constructor for the wrapper:\n12345public async void InitializeModel()&#123;  StorageFile imageRecoModelFile = await StorageFile.GetFileFromApplicationUriAsync(new Uri($”ms-appx:///Data/StreamingAssets/model.onnx“));  imageRecoModel = await Image_RecoModel.CreateImage_RecoModel(imageRecoModelFile);&#125;\nTo be able to use the ONNX model file in the app, you will need to create a specific folder in Unity called exactly StreamingAssets and add the .onnx file there. When you generated the UWP project it will be added in it under /Data/StreaminAssets and its Build Action property will be set to Content. This way you can access it using the Storage API.\n\nInitializing the process of getting Video Frames is straightforward. You can take a look at the code in CreateFrameReader(). It involves some parameter initialization and a call to the static method: MediaCapture.CreateFrameReaderAsync.\nOnce this is done, we call the StartPullFrames method which does all the interesting work.\n12345678910111213141516171819202122232425262728293031323334353637383940414243private void StartPullFrames(MediaFrameReader sender)&#123;  Task.Run(async () =&gt;  &#123;    for (;;)    &#123;      var frameReference = sender.TryAcquireLatestFrame();      var videoFrame = frameReference?.VideoMediaFrame?.GetVideoFrame();      if (videoFrame == null)      &#123;        continue; //ignoring frame      &#125;      var input = new Image_RecoModelInput();      input.data = videoFrame;      if(videoFrame.Direct3DSurface == null)      &#123;        continue; //ignoring frame      &#125;      try      &#123;        Image_RecoModelOutput prediction = await imageRecoModel.EvaluateAsync(input).ConfigureAwait(false);        var classWithHighestProb = prediction.classLabel[];        if (prediction.loss[classWithHighestProb] &gt; 0.5)        &#123;          DisplayText(“I see a “ + classWithHighestProb);        &#125;        else        &#123;          DisplayText(“I see nothing“);        &#125;      &#125;      catch      &#123;        //Log errors      &#125;      await Task.Delay(predictEvery);    &#125;  &#125;);&#125;\nNote: There are a lot of different ways to do this and keep in mind that this is only an example. You should find out the best way to integrate this into your product, which might be different than this approach.\nThis method starts an infinite loop on a separate thread. This loop tries to get the latest frame that was captured by the camera and after a few tests (is the frame null? Is the Direct3DSurface null?) It is given to the EvaluateAsync method of the wrapper.\nNote that in this case, we ask for it to be run synchronously (with ConfigureAwait(false)) so we don’t flood the device with a ton of parallel evaluations.\nOnce we get the result from the model evaluation, we get the name of the class with the highest probability using classLabel[0]. And we check to see if the probability is over 0.5. It is an arbitrary number I picked to not consider detected classes with a too low probability.\nIf you take a look at the wrapper code in Vision.cs, you will notice that the Input and Output classes are the ones that were generated by the command line tool. The only additions I made were initializing the Dictionary with the 2 types of objects available in my custom vision model.\nI tried to optimize the EvaluateAsync method by pre-initializing objects in the static constructor and only binding the output once. It improved a little bit but not significantly enough to say that this is worth it.\n1234567891011121314151617181920212223242526DisplayText(\"Does not work in player.\");  #endif&#125;private void DisplayText(string text)&#123;  textToDisplay = text;  textToDisplayChanged = true;&#125;#if UNITY_WSA &amp;&amp; !UNITY_EDITORpublic async void InitializeModel()&#123;  StorageFile imageRecoModelFile = await StorageFile.GetFileFromApplicationUriAsync(new Uri($\"ms-appx:///Data/StreamingAssets/model.onnx\"));  imageRecoModel = await Image_RecoModel.CreateImage_RecoModel(imageRecoModelFile);&#125;public async void CreateMediaCapture()&#123;  MediaCapture = new MediaCapture();  MediaCaptureInitializationSettings settings = new MediaCaptureInitializationSettings();  settings.StreamingCaptureMode = StreamingCaptureMode.Video;  await MediaCapture.InitializeAsync(settings);  CreateFrameReader();&#125;\nThat’s it! Only a few lines of code using an out of the box feature from Windows and you get an offline Custom Vision model on HoloLens.\nMoving forwardRunning this on a HoloLens is pretty fun: you look at a football, it says “I see a football” then you look at a Rubik’s cube and it says that is it a Rubik’s cube. Ok… maybe it is not the most exciting app but I can tell you that this will help a lot of developers handling offline scenarios! 🙂\nWindows Machine Learning is still new and in preview. We can expect that this will improve a lot in the future. When using a Machine that supports it, Windows ML uses the GPU to do model evaluation. Keep in mind that on HoloLens, it’s only using the CPU.  This means that you must be very cautious about what model you use on this kind of device. Whether it’s a HoloLens, a phone or a tablet, you’ll want to test it and make sure it is fast enough for your scenario. A good idea might be to use the Unity Profiler to understand usage of the CPU and the GPU in your app. Right now, I have no idea if using the GPU will ever be possible on HoloLens for this kind of processing.\nDeep Learning and tools from this big AI family are really the next frontier for AR and VR. In the coming years they are going to be the key component to evolve from good apps to magical experiences.\nCreditsHuge thanks to Jason Fox, Jared Bienz, Nick Landry and Simon Ferquel for the help on reviewing this article and the sample code.\n\nIf you have any question about this blog article, feel free to contact me on twitter: @meulta\n\n&nbsp;\n","dateCreated":"2018-05-18T00:27:52-07:00","dateModified":"2019-01-15T17:58:34-08:00","datePublished":"2018-05-18T00:27:52-07:00","description":"I have an awesome job at Microsoft: I get to work on mixing AI, Cloud and Mixed Reality experiences with partners and other companies. This enables me to work on cool new stuff and see what companies really want to do. Form all of the Microsoft Cognitive Services : Computer Vision is one that bring a lot to Augmented reality experiences .","headline":"Experimenting with Windows Machine Learning and Mixed Reality","image":["http://meulta.github.io/media/logowinml.png"],"mainEntityOfPage":{"@type":"WebPage","@id":"http://yoursite.com/2018/05/18/experimenting-with-windows-machine-learning-and-mixed-reality/"},"publisher":{"@type":"Organization","name":"Etienne Margraff","sameAs":["https://github.com/meulta","https://twitter.com/meulta","https://www.linkedin.com/in/etima/"],"image":"avatar.jpg","logo":{"@type":"ImageObject","url":"avatar.jpg"}},"url":"http://yoursite.com/2018/05/18/experimenting-with-windows-machine-learning-and-mixed-reality/","keywords":"AI, HoloLens, Mixed Reality","thumbnailUrl":"http://meulta.github.io/media/logowinml.png"}</script>
    <meta name="description" content="I have an awesome job at Microsoft: I get to work on mixing AI, Cloud and Mixed Reality experiences with partners and other companies. This enables me to work on cool new stuff and see what companies">
<meta name="keywords" content="AI,HoloLens,Mixed Reality">
<meta property="og:type" content="blog">
<meta property="og:title" content="Experimenting with Windows Machine Learning and Mixed Reality">
<meta property="og:url" content="http://yoursite.com/2018/05/18/experimenting-with-windows-machine-learning-and-mixed-reality/index.html">
<meta property="og:site_name" content="Meulta&#39;s blog">
<meta property="og:description" content="I have an awesome job at Microsoft: I get to work on mixing AI, Cloud and Mixed Reality experiences with partners and other companies. This enables me to work on cool new stuff and see what companies">
<meta property="og:locale" content="en">
<meta property="og:image" content="http://yoursite.com/media/createvisionproject.png">
<meta property="og:image" content="http://yoursite.com/media/training.png">
<meta property="og:image" content="http://yoursite.com/media/export.png">
<meta property="og:image" content="http://yoursite.com/media/streamingassets.png">
<meta property="og:updated_time" content="2019-01-16T01:58:34.052Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Experimenting with Windows Machine Learning and Mixed Reality">
<meta name="twitter:description" content="I have an awesome job at Microsoft: I get to work on mixing AI, Cloud and Mixed Reality experiences with partners and other companies. This enables me to work on cool new stuff and see what companies">
<meta name="twitter:image" content="http://yoursite.com/media/createvisionproject.png">
<meta name="twitter:creator" content="@http:&#x2F;&#x2F;twitter.com&#x2F;meulta">
    
    
        
    
    
        <meta property="og:image" content="http://yoursite.com/assets/images/avatar.jpg"/>
    
    
        <meta property="og:image" content="http://meulta.github.io/media/logowinml.png"/>
        <meta class="swiftype" name="image" data-type="enum" content="http://meulta.github.io/media/logowinml.png" />
    
    
    
    <!--STYLES-->
    <link rel="stylesheet" href="/assets/css/style-c4ozcsklz4kht2pebhp44xorvyverh23toayhn7i6ubrpyedak24hv1v0hyd.min.css">
    <!--STYLES END-->
    

    
</head>

    <body>
        <div id="blog">
            <!-- Define author's picture -->


    
        
            
        
    

<header id="header" data-behavior="4">
    <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
    <div class="header-title">
        <a class="header-title-link" href="/ ">Meulta&#39;s blog</a>
    </div>
    
        
            <a class="header-right-picture " href="#about">
        
        
            <img class="header-picture" src="/assets/images/avatar.jpg" alt="Author&#39;s picture">
        
        </a>
    
</header>

            <!-- Define author's picture -->



        
    

<nav id="sidebar" data-behavior="4">
    <div class="sidebar-container">
        
            <div class="sidebar-profile">
                <a href="/#about">
                    <img class="sidebar-profile-picture" src="/assets/images/avatar.jpg" alt="Author&#39;s picture">
                </a>
                <h4 class="sidebar-profile-name">Etienne Margraff</h4>
                
                    <h5 class="sidebar-profile-bio"><p>I write code and love working with web, AR, VR and Spatial Computing</p>
</h5>
                
            </div>
        
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a class="sidebar-button-link " href="/ " title="Home">
                    
                        <i class="sidebar-button-icon fa fa-home" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Home</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a class="sidebar-button-link " href="/all-categories" title="Categories">
                    
                        <i class="sidebar-button-icon fa fa-bookmark" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Categories</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a class="sidebar-button-link " href="/all-tags" title="Tags">
                    
                        <i class="sidebar-button-icon fa fa-tags" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Tags</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a class="sidebar-button-link " href="/all-archives" title="Archives">
                    
                        <i class="sidebar-button-icon fa fa-archive" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Archives</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a class="sidebar-button-link " href="#about" title="About">
                    
                        <i class="sidebar-button-icon fa fa-question" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">About</span>
                    </a>
            </li>
            
        </ul>
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a class="sidebar-button-link " href="https://github.com/meulta" target="_blank" rel="noopener" title="GitHub">
                    
                        <i class="sidebar-button-icon fab fa-github" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">GitHub</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a class="sidebar-button-link " href="https://twitter.com/meulta" target="_blank" rel="noopener" title="Twitter">
                    
                        <i class="sidebar-button-icon fab fa-twitter" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Twitter</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a class="sidebar-button-link " href="https://www.linkedin.com/in/etima/" target="_blank" rel="noopener" title="LinkedIn">
                    
                        <i class="sidebar-button-icon fab fa-linkedin" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">LinkedIn</span>
                    </a>
            </li>
            
        </ul>
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a class="sidebar-button-link " href="/atom.xml" title="RSS">
                    
                        <i class="sidebar-button-icon fa fa-rss" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">RSS</span>
                    </a>
            </li>
            
        </ul>
        
    </div>
</nav>

            
            <div id="main" data-behavior="4"
                 class="
                        hasCoverMetaIn
                        ">
                
<article class="post">
    
    
        <div class="post-header main-content-wrap text-left">
    
        <h1 class="post-title">
            Experimenting with Windows Machine Learning and Mixed Reality
        </h1>
    
    
        <div class="post-meta">
    <time datetime="2018-05-18T00:27:52-07:00">
	
		    May 18, 2018
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/Uncategorized/">Uncategorized</a>


    
</div>

    
</div>

    
    <div class="post-content markdown">
        <div class="main-content-wrap">
            <p>I have an awesome job at Microsoft: I get to work on mixing AI, Cloud and <a href="https://www.microsoft.com/en-us/windows/windows-mixed-reality" target="_blank" rel="noopener">Mixed Reality</a> experiences with partners and other companies. This enables me to work on cool new stuff and see what companies really want to do. Form all of the Microsoft <a href="https://azure.microsoft.com/en-us/services/cognitive-services/" target="_blank" rel="noopener">Cognitive Services</a> : Computer Vision is one that bring a lot to Augmented reality experiences .<br><a id="more"></a><br>We&#8217;ve already helped a lot of companies using the <a href="https://customvision.ai/" target="_blank" rel="noopener">Custom Vision online service</a> which you can access via a REST API. The service tells you what’s in the image by providing a list of tags along with their probability ratio. A new feature was added more recently to show you <em>where</em> these objects are located in the image, thanks to the new Object Detection model with bounding boxes.</p>
<p>The only piece which was missing is offline support. When you don’t have any internet connectivity, these services are not very useful anymore. When detecting that the device you have is offline, you have two possibilities:</p>
<ul>
<li>Disabling the features that require this REST API.</li>
<li>Trying to perform object recognition locally, on the device.</li>
</ul>
<p>We recently announced <a href="https://docs.microsoft.com/en-us/windows/uwp/machine-learning" target="_blank" rel="noopener">Windows Machine Learning</a>. It is a service in Windows accessible through a set of APIs when you are building Universal Windows Platform (UWP) applications. This service was released at the end of April 2018 in the brand-new Windows update. The good news is: this Windows update is also available on <a href="https://www.microsoft.com/en-us/hololens" target="_blank" rel="noopener">HoloLens</a>!</p>
<p>This Windows ML feature requires a trained model in the <a href="https://onnx.ai/" target="_blank" rel="noopener">ONNX format</a>. ONNX is an initiative from companies such as AWS, Facebook and Microsoft to create an open format to represent deep learning models. One way to create an ONNX model today is to convert it from one that already exists. Guess what? Custom Vision gives you the feature of exporting a trained model in ONNX (for some of the model types available).</p>
<p>I recently worked with a company to try and mix all of this and be able to run a Custom Vision model on HoloLens.</p>
<p><strong>It worked! Here is how we did it</strong> (using a demo project as an example. You can get the full code here: <a href="https://aka.ms/mr-winml-code" target="_blank" rel="noopener">https://aka.ms/mr-winml-code</a>).</p>
<div style="width: 640px;" class="wp-video"><br>  <video class="wp-video-shortcode" id="video-310-7" width="640" height="200" loop="1" autoplay="1" preload="auto" controls="controls"><source type="video/mp4" src="/media/winml.mp4?_=7"><a href="/media/winml.mp4">winml.mp4</a></video><br></div>

<h2 id="Use-Custom-Vision-to-create-an-ONNX-model"><a href="#Use-Custom-Vision-to-create-an-ONNX-model" class="headerlink" title="Use Custom Vision to create an ONNX model"></a>Use Custom Vision to create an ONNX model</h2><p>Custom Vision (<a href="http://www.customvision.ai/" target="_blank" rel="noopener">customvision.ai</a>) is one of the Cognitive Services. These are a set of Machine Learning ready-to-use models. The idea is simple: you get the power of Deep Learning and other algorithms without even having to go through the challenge of creating and training the model yourself. If you know how to call a REST API, you know how to use Cognitive Services. Some of these services are customizable. In the Vision API, we have a “Custom” version that you can train by uploading pictures and tagging them.</p>
<p style="text-align: left;"><br>  <em><strong>Note: </strong>We also recently released a preview of the Object Detection model. It brings you a customizable Vision service WITH bounding boxes! So now not only do you know what is in the picture, you can also know where it is. It is very helpful in Augmented Reality apps as it enables a way to display data on top of objects.</em><br></p>

<p>There is a very comprehensive guide <a href="https://docs.microsoft.com/en-us/azure/cognitive-services/custom-vision-service/getting-started-build-a-classifier" target="_blank" rel="noopener">available here</a> explaining how to create a classifier so I am not going to go through it here in detail. Here are some interesting things for you to consider:</p>
<p>First, pick a compact model. If you want to export as ONNX, the model as to be marked as <strong>(compact)</strong>, otherwise you won’t be able to. This makes sense in a way that these models are designed to run offline on devices that do not have a lot of power. It is a good way to insure you have usable exported models.</p>
<p><img class="alignnone wp-image-319" src="/media/createvisionproject.png" alt="" width="644" height="487" sizes="(max-width: 644px) 100vw, 644px"></p>
<p>Once you’ve picked the model you need, you just have to train it. In my examples, I am training it using photos of a Rubik’s cube and a Seahawks football (for no particular reason, these were just the first things that I found in my office when writing this sample &#x1f60a; ).</p>
<p><img class="alignnone wp-image-324" src="/media/training.png" alt="" width="887" height="489" sizes="(max-width: 887px) 100vw, 887px"></p>
<p>You can start with as few as 10 pictures for each tag and the service will surprise you with its ability to recognize these very accurately. Of course, you can refine the model by adding more pictures with different lightnings, coloring, etc.</p>
<p>Exporting is very easy, go in the <strong>Performance</strong> tab and click on the <strong>Export</strong> button. You can choose the format you want and in this case, we need ONNX.</p>
<p><img class="alignnone wp-image-325" src="/media/export.png" alt="" width="634" height="434" sizes="(max-width: 634px) 100vw, 634px"></p>
<p>That’s it. We now have a model we can use offline!</p>
<h2 id="Generate-the-Windows-ML-wrapper"><a href="#Generate-the-Windows-ML-wrapper" class="headerlink" title="Generate the Windows ML wrapper"></a>Generate the Windows ML wrapper</h2><p>Now that we have a model, we need to write some code that is going to make use of it. You can find everything you need about Windows Machine Learning in the <a href="https://docs.microsoft.com/en-us/windows/uwp/machine-learning/" target="_blank" rel="noopener">documentation</a> but let’s get through the main actions you need to perform.</p>
<p>When I say that we need to write some code, it is not completely accurate. We will <em>generate</em> a wrapper to use the ONNX model in UWP. The only code we need to write ourselves is the one that gets the image from the webcam and pass it to the wrapper.</p>
<p>The Windows ML SDK comes with a tool named MLGen.exe. This command line tool helps you generate a wrapper for an ONNX file. It is located where you installed the Windows SDK. On my computer, which has a standard install, here where it is:</p>
<pre>C:\Program Files (x86)\Windows Kits\10\bin\10.0.17125.0\x86\mlgen.exe</pre>

<p>The command line to run is straightforward:</p>
<pre>mlgen -i INPUT-FILE -l LANGUAGE -n NAMESPACE [-o OUTPUT-FILE]</pre>

<ul>
<li><strong>INPUT-FILE</strong> is the ONNX file.</li>
<li><strong>LANGUAGE</strong> is the programming language (CS in this case).</li>
<li><strong>NAMESPACE</strong> is the namespace used in the CSharp file. This should be something usable in your project, but you can change it later.</li>
<li><strong>OUTPUT-FILE</strong> is the file that is going to be created (a .cs one, in this case).</li>
</ul>
<p>A generated wrapper contains 3 classes (considering that “ModelName” is the name of your model):</p>
<ul>
<li><strong>ModelName</strong>_<strong>Input</strong>: a structure to hold the input data. In Vision related model, it is a <strong>VideoFrame</strong> which is easy to get from the Camera in UWP.</li>
<li><strong>ModelName</strong>_<strong>Ouput</strong>: a structure used by the wrapper to give you the output.</li>
<li><strong>ModelName</strong>: a class responsible for running the model evaluation. It contains a static constructor and an asynchronous method to evaluate  the model prediction.</li>
</ul>
<p>Here is an example of generated code:</p>
<figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">using</span> System;</span><br><span class="line"><span class="keyword">using</span> System.Collections.Generic;</span><br><span class="line"><span class="keyword">using</span> System.Threading.Tasks;</span><br><span class="line"><span class="keyword">using</span> Windows.Media;</span><br><span class="line"><span class="keyword">using</span> Windows.Storage;</span><br><span class="line"><span class="keyword">using</span> Windows.AI.MachineLearning.Preview;</span><br><span class="line"></span><br><span class="line"><span class="keyword">namespace</span> <span class="title">MyNamespace</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">sealed</span> <span class="keyword">class</span> <span class="title">MyModelInput</span></span><br><span class="line">  &#123;</span><br><span class="line">    <span class="keyword">public</span> VideoFrame data &#123; <span class="keyword">get</span>; <span class="keyword">set</span>; &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">sealed</span> <span class="keyword">class</span> <span class="title">MyModelOutput</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="keyword">public</span> IList&lt;<span class="keyword">string</span>&gt; classLabel &#123; <span class="keyword">get</span>; <span class="keyword">set</span>; &#125;</span><br><span class="line">  <span class="keyword">public</span> IDictionary&lt;<span class="keyword">string</span>, <span class="keyword">float</span>&gt; loss &#123; <span class="keyword">get</span>; <span class="keyword">set</span>; &#125;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">MyModelOutput</span>(<span class="params"></span>)</span></span><br><span class="line"><span class="function"></span>  &#123;</span><br><span class="line">    <span class="keyword">this</span>.classLabel = <span class="keyword">new</span> List&lt;<span class="keyword">string</span>&gt;();</span><br><span class="line">    <span class="keyword">this</span>.loss = <span class="keyword">new</span> Dictionary&lt;<span class="keyword">string</span>, <span class="keyword">float</span>&gt;();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">sealed</span> <span class="keyword">class</span> <span class="title">MyModel</span></span><br><span class="line">  &#123;</span><br><span class="line">    <span class="keyword">private</span> LearningModelPreview learningModel;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">async</span> Task&lt;MyModel&gt; <span class="title">CreateMyModel</span>(<span class="params">StorageFile file</span>)</span></span><br><span class="line"><span class="function"></span>    &#123;</span><br><span class="line">      LearningModelPreview learningModel = <span class="keyword">await</span> LearningModelPreview.LoadModelFromStorageFileAsync(file);</span><br><span class="line">      MyModel model = <span class="keyword">new</span> MyModel();</span><br><span class="line">      model.learningModel = learningModel;</span><br><span class="line">      <span class="keyword">return</span> model;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">async</span> Task&lt;MyModelOutput&gt; <span class="title">EvaluateAsync</span>(<span class="params">MyModelInput input</span>)</span> &#123;</span><br><span class="line">      MyModelOutput output = <span class="keyword">new</span> MyModelOutput();</span><br><span class="line">      LearningModelBindingPreview binding = <span class="keyword">new</span> LearningModelBindingPreview(learningModel);</span><br><span class="line">      binding.Bind(“data“, input.data);</span><br><span class="line">      binding.Bind(“classLabel“, output.classLabel);</span><br><span class="line">      binding.Bind(“loss“, output.loss);</span><br><span class="line">      LearningModelEvaluationResultPreview evalResult = <span class="keyword">await</span> learningModel.EvaluateAsync(binding, <span class="keyword">string</span>.Empty);</span><br><span class="line">      <span class="keyword">return</span> output;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p style="text-align: left;"><br>  <em><strong>Note</strong>: When exporting from the Custom Vision service, the name of the model is a generated one. You might have to change it to something that is more human readable.</em><br></p>

<p>As you can see in this example, the generation tool created an output structure with 2 parameters:</p>
<ul>
<li><strong>classLabel</strong>: the labels with the highest probability.</li>
<li><strong>loss</strong>: a dictionary with all labels and their respective probability.</li>
</ul>
<p>The dictionary is not initialized. If you leave it this way you will get an Exception when running the code. You have to initialize it using the labels you set in the Custom Vision portal. Here is how it looks like for mine:</p>
<figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">this</span>.loss = <span class="keyword">new</span> Dictionary&lt;<span class="keyword">string</span>, <span class="keyword">float</span>&gt;()</span><br><span class="line">&#123;</span><br><span class="line">  &#123; <span class="string">"football"</span>, <span class="number">0f</span> &#125;,</span><br><span class="line">  &#123; <span class="string">"rubikscube"</span>, <span class="number">0f</span> &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>Awesome, we now have a usable wrapper. Let see how we can integrate this in a Mixed Reality app.</p>
<h2 id="Integrate-in-your-Mixed-Reality-app"><a href="#Integrate-in-your-Mixed-Reality-app" class="headerlink" title="Integrate in your Mixed Reality app"></a><strong>Integrate in your Mixed Reality app</strong></h2><p>The best way to start integrating your wrapper and ONNX model in an application is to look at the <a href="https://docs.microsoft.com/en-us/windows/uwp/machine-learning/samples" target="_blank" rel="noopener">samples</a> that the team is providing. They will guide you through everything you need: starting the Camera, collecting <strong>VideoFrames</strong>, sending these to the wrapper and getting the result.</p>
<p>When you are creating a Windows Mixed Reality application and, more specifically, a HoloLens app, you use Unity to setup your scene with 3D objects as needed along with C# scripts. This Unity project is then built (or “exported”) as a Visual Studio solution containing a Universal Windows Platform (UWP) project.</p>
<p style="text-align: left;"><br>  <em><strong>Note</strong>: In theory, a Unity project is meant to be exported to different platforms. We usually try to keep the code as portable as possible. In this specific case we are going to use APIs that are specific to Windows to run the evaluation of the ONNX model. To make sure this platform specific code does not generate errors in Unity, we will wrap it inside conditional compilation keyword. This will tell Unity and Visual Studio: “don’t try to parse / compile this unless you are in UWP”. If you want to run this code on another environment you will have to (at least) add another conditional compilation switch to add the code specific to this platform.</em><br></p>

<p><span style="display: inline !important; float: none; background-color: transparent; color: #333333; cursor: text; font-family: 'Roboto',Helvetica,sans-serif; font-size: 16px; font-style: normal; font-variant: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-decoration: none; text-indent: 0px; text-transform: none; -webkit-text-stroke-width: 0px; white-space: normal; word-spacing: 0px;">You can get the full code for this sample here: <a href="https://aka.ms/mr-winml-code" target="_blank" rel="noopener">https://aka.ms/mr-winml-code</a> </span></p>
<p>In the sample project, there is a Game Object with no graphical representation. It is called <strong>ScriptHolder</strong> and its only role is to have some scripts attached to run code at specific moments during execution. This object has a script named <strong>Scene Startup</strong> attached to it. This script contains all the code needed to create the ONNX wrapper, get the <strong>VideoFrames</strong> and display the result of prediction.</p>
<p>The <strong>Start</strong> method in the standard Unity MonoBehaviour is called automatically when the object appears in the scene (i.e. when the application is starting). This will:</p>
<ul>
<li>Get an instance to the label where we are going to display results to.</li>
<li>Create and initialize the <strong>MediaCapture</strong> object to start collecting frames from the camera.</li>
<li>Initialize the wrapper for the ONNX model.</li>
</ul>
<p>Initializing the model is pretty easy, you just load the file from the local storage and call the static constructor for the wrapper:</p>
<figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">async</span> <span class="keyword">void</span> <span class="title">InitializeModel</span>(<span class="params"></span>)</span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  StorageFile imageRecoModelFile = <span class="keyword">await</span> StorageFile.GetFileFromApplicationUriAsync(<span class="keyword">new</span> Uri($”ms-appx:<span class="comment"><span class="doctag">///</span>Data/StreamingAssets/model.onnx“));</span></span><br><span class="line">  imageRecoModel = <span class="keyword">await</span> Image_RecoModel.CreateImage_RecoModel(imageRecoModelFile);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>To be able to use the ONNX model file in the app, you will need to create a specific folder in Unity called exactly <strong>StreamingAssets</strong> and add the .onnx file there. When you generated the UWP project it will be added in it under <em>/Data/StreaminAssets</em> and its <strong>Build Action</strong> property will be set to <strong>Content</strong>. This way you can access it using the Storage API.</p>
<p><img class="alignnone wp-image-331" src="/media/streamingassets.png" alt="" width="483" height="205" sizes="(max-width: 483px) 100vw, 483px"></p>
<p>Initializing the process of getting Video Frames is straightforward. You can take a look at the code in <strong>CreateFrameReader()</strong>. It involves some parameter initialization and a call to the static method: <strong>MediaCapture.CreateFrameReaderAsync</strong>.</p>
<p>Once this is done, we call the <strong>StartPullFrames</strong> method which does all the interesting work.</p>
<figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">StartPullFrames</span>(<span class="params">MediaFrameReader sender</span>)</span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  Task.Run(<span class="keyword">async</span> () =&gt;</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="keyword">for</span> (;;)</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="keyword">var</span> frameReference = sender.TryAcquireLatestFrame();</span><br><span class="line">      <span class="keyword">var</span> videoFrame = frameReference?.VideoMediaFrame?.GetVideoFrame();</span><br><span class="line">      <span class="keyword">if</span> (videoFrame == <span class="literal">null</span>)</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="keyword">continue</span>; <span class="comment">//ignoring frame</span></span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">var</span> input = <span class="keyword">new</span> Image_RecoModelInput();</span><br><span class="line">      input.data = videoFrame;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span>(videoFrame.Direct3DSurface == <span class="literal">null</span>)</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="keyword">continue</span>; <span class="comment">//ignoring frame</span></span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">try</span></span><br><span class="line">      &#123;</span><br><span class="line">        Image_RecoModelOutput prediction = <span class="keyword">await</span> imageRecoModel.EvaluateAsync(input).ConfigureAwait(<span class="literal">false</span>);</span><br><span class="line">        <span class="keyword">var</span> classWithHighestProb = prediction.classLabel[];</span><br><span class="line">        <span class="keyword">if</span> (prediction.loss[classWithHighestProb] &gt; <span class="number">0.5</span>)</span><br><span class="line">        &#123;</span><br><span class="line">          DisplayText(“I see a “ + classWithHighestProb);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        &#123;</span><br><span class="line">          DisplayText(“I see nothing“);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">catch</span></span><br><span class="line">      &#123;</span><br><span class="line">        <span class="comment">//Log errors</span></span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">await</span> Task.Delay(predictEvery);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><em>Note: There are a lot of different ways to do this and keep in mind that this is only an example. You should find out the best way to integrate this into your product, which might be different than this approach.</em></p>
<p>This method starts an infinite loop on a separate thread. This loop tries to get the latest frame that was captured by the camera and after a few tests (is the frame null? Is the <strong>Direct3DSurface</strong> null?) It is given to the <strong>EvaluateAsync</strong> method of the wrapper.</p>
<p>Note that in this case, we ask for it to be run synchronously (with <strong>ConfigureAwait(false)</strong>) so we don’t flood the device with a ton of parallel evaluations.</p>
<p>Once we get the result from the model evaluation, we get the name of the class with the highest probability using <strong>classLabel[0]</strong>. And we check to see if the probability is over 0.5. It is an arbitrary number I picked to not consider detected classes with a too low probability.</p>
<p>If you take a look at the wrapper code in <a href="https://github.com/Microsoft/mixedreality-azure-samples/blob/42289b5c9b49aed5c1e5e6194ad6aa41a66d5ba7/Standalone-Samples/WindowsML-CustomVision-Hololens/Assets/Scripts/Vision.cs#L26" target="_blank" rel="noopener"><strong>Vision.cs</strong></a>, you will notice that the <strong>Input</strong> and <strong>Output</strong> classes are the ones that were generated by the command line tool. The only additions I made were initializing the <strong>Dictionary</strong> with the 2 types of objects available in my custom vision model.</p>
<p>I tried to optimize the <strong>EvaluateAsync</strong> method by pre-initializing objects in the static constructor and only binding the output once. It improved a little bit but not significantly enough to say that this is worth it.</p>
<figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">DisplayText(<span class="string">"Does not work in player."</span>);</span><br><span class="line">  <span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">DisplayText</span>(<span class="params"><span class="keyword">string</span> text</span>)</span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  textToDisplay = text;</span><br><span class="line">  textToDisplayChanged = <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">if</span> UNITY_WSA &amp;&amp; !UNITY_EDITOR</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">async</span> <span class="keyword">void</span> <span class="title">InitializeModel</span>(<span class="params"></span>)</span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  StorageFile imageRecoModelFile = <span class="keyword">await</span> StorageFile.GetFileFromApplicationUriAsync(<span class="keyword">new</span> Uri(<span class="string">$"ms-appx:///Data/StreamingAssets/model.onnx"</span>));</span><br><span class="line">  imageRecoModel = <span class="keyword">await</span> Image_RecoModel.CreateImage_RecoModel(imageRecoModelFile);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">async</span> <span class="keyword">void</span> <span class="title">CreateMediaCapture</span>(<span class="params"></span>)</span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  MediaCapture = <span class="keyword">new</span> MediaCapture();</span><br><span class="line">  MediaCaptureInitializationSettings settings = <span class="keyword">new</span> MediaCaptureInitializationSettings();</span><br><span class="line">  settings.StreamingCaptureMode = StreamingCaptureMode.Video;</span><br><span class="line">  <span class="keyword">await</span> MediaCapture.InitializeAsync(settings);</span><br><span class="line"></span><br><span class="line">  CreateFrameReader();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>That’s it! Only a few lines of code using an out of the box feature from Windows and you get an offline Custom Vision model on HoloLens.</p>
<h2 id="Moving-forward"><a href="#Moving-forward" class="headerlink" title="Moving forward"></a>Moving forward</h2><p>Running this on a HoloLens is pretty fun: you look at a football, it says “I see a football” then you look at a Rubik’s cube and it says that is it a Rubik’s cube. Ok… maybe it is not the most exciting app but I can tell you that this will help a lot of developers handling offline scenarios! 🙂</p>
<p>Windows Machine Learning is still new and in preview. We can expect that this will improve a lot in the future. When using a Machine that supports it, Windows ML uses the GPU to do model evaluation. Keep in mind that on HoloLens, it’s only using the CPU.  This means that you must be very cautious about what model you use on this kind of device. Whether it’s a HoloLens, a phone or a tablet, you’ll want to test it and make sure it is fast enough for your scenario. A good idea might be to use the <a href="https://docs.unity3d.com/Manual/Profiler.html" target="_blank" rel="noopener">Unity Profiler</a> to understand usage of the CPU and the GPU in your app. Right now, I have no idea if using the GPU will ever be possible on HoloLens for this kind of processing.</p>
<p>Deep Learning and tools from this big AI family are really the next frontier for AR and VR. In the coming years they are going to be the key component to evolve from good apps to magical experiences.</p>
<h2 id="Credits"><a href="#Credits" class="headerlink" title="Credits"></a><strong>Credits</strong></h2><p>Huge thanks to <a href="https://twitter.com/JasonGFox" target="_blank" rel="noopener">Jason Fox</a>, <a href="https://twitter.com/jbienz" target="_blank" rel="noopener">Jared Bienz</a>, <a href="https://twitter.com/ActiveNick" target="_blank" rel="noopener">Nick Landry</a> and <a href="https://twitter.com/sferquel" target="_blank" rel="noopener">Simon Ferquel</a> for the help on reviewing this article and the sample code.</p>
<blockquote>
<p>If you have any question about this blog article, feel free to contact me on twitter: <a href="https://twitter.com/meulta" target="_blank" rel="noopener">@meulta</a></p>
</blockquote>
<p>&nbsp;</p>

            

        </div>
    </div>
    <div id="post-footer" class="post-footer main-content-wrap">
        
            <div class="post-footer-tags">
                <span class="text-color-light text-small">TAGGED IN</span><br>
                
    <a class="tag tag--primary tag--small t-link" href="/tags/AI/">AI</a> <a class="tag tag--primary tag--small t-link" href="/tags/HoloLens/">HoloLens</a> <a class="tag tag--primary tag--small t-link" href="/tags/Mixed-Reality/">Mixed Reality</a>

            </div>
        
        
            <div class="post-actions-wrap">
    <nav>
        <ul class="post-actions post-action-nav">
            <li class="post-action">
                
                    <a class="post-action-btn btn btn--disabled">
                
                    <i class="fa fa-angle-left" aria-hidden="true"></i>
                    <span class="hide-xs hide-sm text-small icon-ml">PREVIOUS</span>
                </a>
            </li>
            <li class="post-action">
                
                    
                    <a class="post-action-btn btn btn--default tooltip--top" href="/2017/04/17/bot-framework-web-chat-and-push-notifications/" data-tooltip="Bot framework, web chat and push notifications" aria-label="NEXT: Bot framework, web chat and push notifications">
                
                    <span class="hide-xs hide-sm text-small icon-mr">NEXT</span>
                    <i class="fa fa-angle-right" aria-hidden="true"></i>
                </a>
            </li>
        </ul>
    </nav>
    <ul class="post-actions post-action-share">
        <li class="post-action hide-lg hide-md hide-sm">
            <a class="post-action-btn btn btn--default btn-open-shareoptions" href="#btn-open-shareoptions" aria-label="Share this post">
                <i class="fa fa-share-alt" aria-hidden="true"></i>
            </a>
        </li>
        
            
            
            <li class="post-action hide-xs">
                <a class="post-action-btn btn btn--default" target="new" href="https://www.facebook.com/sharer/sharer.php?u=http://yoursite.com/2018/05/18/experimenting-with-windows-machine-learning-and-mixed-reality/" title="Share on Facebook">
                    <i class="fab fa-facebook" aria-hidden="true"></i>
                </a>
            </li>
        
            
            
            <li class="post-action hide-xs">
                <a class="post-action-btn btn btn--default" target="new" href="https://twitter.com/intent/tweet?text=http://yoursite.com/2018/05/18/experimenting-with-windows-machine-learning-and-mixed-reality/" title="Share on Twitter">
                    <i class="fab fa-twitter" aria-hidden="true"></i>
                </a>
            </li>
        
            
            
            <li class="post-action hide-xs">
                <a class="post-action-btn btn btn--default" target="new" href="https://plus.google.com/share?url=http://yoursite.com/2018/05/18/experimenting-with-windows-machine-learning-and-mixed-reality/" title="Share on Google+">
                    <i class="fab fa-google-plus" aria-hidden="true"></i>
                </a>
            </li>
        
        
            
        
        <li class="post-action">
            
                <a class="post-action-btn btn btn--default" href="#" aria-label="Back to top">
            
                <i class="fa fa-list" aria-hidden="true"></i>
            </a>
        </li>
    </ul>
</div>


        
        
            
        
    </div>
</article>



                <footer id="footer" class="main-content-wrap">
    <span class="copyrights">
        Copyrights &copy; 2019 Etienne Margraff. All Rights Reserved.
    </span>
</footer>

            </div>
            
                <div id="bottom-bar" class="post-bottom-bar" data-behavior="4">
                    <div class="post-actions-wrap">
    <nav>
        <ul class="post-actions post-action-nav">
            <li class="post-action">
                
                    <a class="post-action-btn btn btn--disabled">
                
                    <i class="fa fa-angle-left" aria-hidden="true"></i>
                    <span class="hide-xs hide-sm text-small icon-ml">PREVIOUS</span>
                </a>
            </li>
            <li class="post-action">
                
                    
                    <a class="post-action-btn btn btn--default tooltip--top" href="/2017/04/17/bot-framework-web-chat-and-push-notifications/" data-tooltip="Bot framework, web chat and push notifications" aria-label="NEXT: Bot framework, web chat and push notifications">
                
                    <span class="hide-xs hide-sm text-small icon-mr">NEXT</span>
                    <i class="fa fa-angle-right" aria-hidden="true"></i>
                </a>
            </li>
        </ul>
    </nav>
    <ul class="post-actions post-action-share">
        <li class="post-action hide-lg hide-md hide-sm">
            <a class="post-action-btn btn btn--default btn-open-shareoptions" href="#btn-open-shareoptions" aria-label="Share this post">
                <i class="fa fa-share-alt" aria-hidden="true"></i>
            </a>
        </li>
        
            
            
            <li class="post-action hide-xs">
                <a class="post-action-btn btn btn--default" target="new" href="https://www.facebook.com/sharer/sharer.php?u=http://yoursite.com/2018/05/18/experimenting-with-windows-machine-learning-and-mixed-reality/" title="Share on Facebook">
                    <i class="fab fa-facebook" aria-hidden="true"></i>
                </a>
            </li>
        
            
            
            <li class="post-action hide-xs">
                <a class="post-action-btn btn btn--default" target="new" href="https://twitter.com/intent/tweet?text=http://yoursite.com/2018/05/18/experimenting-with-windows-machine-learning-and-mixed-reality/" title="Share on Twitter">
                    <i class="fab fa-twitter" aria-hidden="true"></i>
                </a>
            </li>
        
            
            
            <li class="post-action hide-xs">
                <a class="post-action-btn btn btn--default" target="new" href="https://plus.google.com/share?url=http://yoursite.com/2018/05/18/experimenting-with-windows-machine-learning-and-mixed-reality/" title="Share on Google+">
                    <i class="fab fa-google-plus" aria-hidden="true"></i>
                </a>
            </li>
        
        
            
        
        <li class="post-action">
            
                <a class="post-action-btn btn btn--default" href="#" aria-label="Back to top">
            
                <i class="fa fa-list" aria-hidden="true"></i>
            </a>
        </li>
    </ul>
</div>


                </div>
                
    <div id="share-options-bar" class="share-options-bar" data-behavior="4">
        <i id="btn-close-shareoptions" class="fa fa-times"></i>
        <ul class="share-options">
            
                
                
                <li class="share-option">
                    <a class="share-option-btn" target="new" href="https://www.facebook.com/sharer/sharer.php?u=http://yoursite.com/2018/05/18/experimenting-with-windows-machine-learning-and-mixed-reality/">
                        <i class="fab fa-facebook" aria-hidden="true"></i><span>Share on Facebook</span>
                    </a>
                </li>
            
                
                
                <li class="share-option">
                    <a class="share-option-btn" target="new" href="https://twitter.com/intent/tweet?text=http://yoursite.com/2018/05/18/experimenting-with-windows-machine-learning-and-mixed-reality/">
                        <i class="fab fa-twitter" aria-hidden="true"></i><span>Share on Twitter</span>
                    </a>
                </li>
            
                
                
                <li class="share-option">
                    <a class="share-option-btn" target="new" href="https://plus.google.com/share?url=http://yoursite.com/2018/05/18/experimenting-with-windows-machine-learning-and-mixed-reality/">
                        <i class="fab fa-google-plus" aria-hidden="true"></i><span>Share on Google+</span>
                    </a>
                </li>
            
        </ul>
    </div>


            
        </div>
        


    
        
    

<div id="about">
    <div id="about-card">
        <div id="about-btn-close">
            <i class="fa fa-times"></i>
        </div>
        
            <img id="about-card-picture" src="/assets/images/avatar.jpg" alt="Author&#39;s picture">
        
            <h4 id="about-card-name">Etienne Margraff</h4>
        
            <div id="about-card-bio"><p>I write code and love working with web, AR, VR and Spatial Computing</p>
</div>
        
        
            <div id="about-card-job">
                <i class="fa fa-briefcase"></i>
                <br>
                <p>Software Engineering Lead</p>

            </div>
        
        
            <div id="about-card-location">
                <i class="fa fa-map-marker-alt"></i>
                <br>
                Seattle, WA
            </div>
        
    </div>
</div>

        
        
<div id="cover" style="background-image:url('/assets/images/cover-v1.2.0.jpg');"></div>
        <!--SCRIPTS-->
<script src="/assets/js/script-dbd16rvloemmuxdzniplmnxxvwoz24eya9wol0b7vvmlokgqsjivmb8dnscy.min.js"></script>
<!--SCRIPTS END-->

    



    </body>
</html>
